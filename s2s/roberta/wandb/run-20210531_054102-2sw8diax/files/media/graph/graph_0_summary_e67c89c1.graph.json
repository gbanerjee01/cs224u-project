{"format": "torch", "nodes": [{"name": "encoder", "id": 140043330256008, "class_name": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)", "parameters": [["embeddings.word_embeddings.weight", [50265, 768]], ["embeddings.position_embeddings.weight", [514, 768]], ["embeddings.token_type_embeddings.weight", [1, 768]], ["embeddings.LayerNorm.weight", [768]], ["embeddings.LayerNorm.bias", [768]], ["encoder.layer.0.attention.self.query.weight", [768, 768]], ["encoder.layer.0.attention.self.query.bias", [768]], ["encoder.layer.0.attention.self.key.weight", [768, 768]], ["encoder.layer.0.attention.self.key.bias", [768]], ["encoder.layer.0.attention.self.value.weight", [768, 768]], ["encoder.layer.0.attention.self.value.bias", [768]], ["encoder.layer.0.attention.output.dense.weight", [768, 768]], ["encoder.layer.0.attention.output.dense.bias", [768]], ["encoder.layer.0.attention.output.LayerNorm.weight", [768]], ["encoder.layer.0.attention.output.LayerNorm.bias", [768]], ["encoder.layer.0.intermediate.dense.weight", [3072, 768]], ["encoder.layer.0.intermediate.dense.bias", [3072]], ["encoder.layer.0.output.dense.weight", [768, 3072]], ["encoder.layer.0.output.dense.bias", [768]], ["encoder.layer.0.output.LayerNorm.weight", [768]], ["encoder.layer.0.output.LayerNorm.bias", [768]], ["encoder.layer.1.attention.self.query.weight", [768, 768]], ["encoder.layer.1.attention.self.query.bias", [768]], ["encoder.layer.1.attention.self.key.weight", [768, 768]], ["encoder.layer.1.attention.self.key.bias", [768]], ["encoder.layer.1.attention.self.value.weight", [768, 768]], ["encoder.layer.1.attention.self.value.bias", [768]], ["encoder.layer.1.attention.output.dense.weight", [768, 768]], ["encoder.layer.1.attention.output.dense.bias", [768]], ["encoder.layer.1.attention.output.LayerNorm.weight", [768]], ["encoder.layer.1.attention.output.LayerNorm.bias", [768]], ["encoder.layer.1.intermediate.dense.weight", [3072, 768]], ["encoder.layer.1.intermediate.dense.bias", [3072]], ["encoder.layer.1.output.dense.weight", [768, 3072]], ["encoder.layer.1.output.dense.bias", [768]], ["encoder.layer.1.output.LayerNorm.weight", [768]], ["encoder.layer.1.output.LayerNorm.bias", [768]], ["encoder.layer.2.attention.self.query.weight", [768, 768]], ["encoder.layer.2.attention.self.query.bias", [768]], ["encoder.layer.2.attention.self.key.weight", [768, 768]], ["encoder.layer.2.attention.self.key.bias", [768]], ["encoder.layer.2.attention.self.value.weight", [768, 768]], ["encoder.layer.2.attention.self.value.bias", [768]], ["encoder.layer.2.attention.output.dense.weight", [768, 768]], ["encoder.layer.2.attention.output.dense.bias", [768]], ["encoder.layer.2.attention.output.LayerNorm.weight", [768]], ["encoder.layer.2.attention.output.LayerNorm.bias", [768]], ["encoder.layer.2.intermediate.dense.weight", [3072, 768]], ["encoder.layer.2.intermediate.dense.bias", [3072]], ["encoder.layer.2.output.dense.weight", [768, 3072]], ["encoder.layer.2.output.dense.bias", [768]], ["encoder.layer.2.output.LayerNorm.weight", [768]], ["encoder.layer.2.output.LayerNorm.bias", [768]], ["encoder.layer.3.attention.self.query.weight", [768, 768]], ["encoder.layer.3.attention.self.query.bias", [768]], ["encoder.layer.3.attention.self.key.weight", [768, 768]], ["encoder.layer.3.attention.self.key.bias", [768]], ["encoder.layer.3.attention.self.value.weight", [768, 768]], ["encoder.layer.3.attention.self.value.bias", [768]], ["encoder.layer.3.attention.output.dense.weight", [768, 768]], ["encoder.layer.3.attention.output.dense.bias", [768]], ["encoder.layer.3.attention.output.LayerNorm.weight", [768]], ["encoder.layer.3.attention.output.LayerNorm.bias", [768]], ["encoder.layer.3.intermediate.dense.weight", [3072, 768]], ["encoder.layer.3.intermediate.dense.bias", [3072]], ["encoder.layer.3.output.dense.weight", [768, 3072]], ["encoder.layer.3.output.dense.bias", [768]], ["encoder.layer.3.output.LayerNorm.weight", [768]], ["encoder.layer.3.output.LayerNorm.bias", [768]], ["encoder.layer.4.attention.self.query.weight", [768, 768]], ["encoder.layer.4.attention.self.query.bias", [768]], ["encoder.layer.4.attention.self.key.weight", [768, 768]], ["encoder.layer.4.attention.self.key.bias", [768]], ["encoder.layer.4.attention.self.value.weight", [768, 768]], ["encoder.layer.4.attention.self.value.bias", [768]], ["encoder.layer.4.attention.output.dense.weight", [768, 768]], ["encoder.layer.4.attention.output.dense.bias", [768]], ["encoder.layer.4.attention.output.LayerNorm.weight", [768]], ["encoder.layer.4.attention.output.LayerNorm.bias", [768]], ["encoder.layer.4.intermediate.dense.weight", [3072, 768]], ["encoder.layer.4.intermediate.dense.bias", [3072]], ["encoder.layer.4.output.dense.weight", [768, 3072]], ["encoder.layer.4.output.dense.bias", [768]], ["encoder.layer.4.output.LayerNorm.weight", [768]], ["encoder.layer.4.output.LayerNorm.bias", [768]], ["encoder.layer.5.attention.self.query.weight", [768, 768]], ["encoder.layer.5.attention.self.query.bias", [768]], ["encoder.layer.5.attention.self.key.weight", [768, 768]], ["encoder.layer.5.attention.self.key.bias", [768]], ["encoder.layer.5.attention.self.value.weight", [768, 768]], ["encoder.layer.5.attention.self.value.bias", [768]], ["encoder.layer.5.attention.output.dense.weight", [768, 768]], ["encoder.layer.5.attention.output.dense.bias", [768]], ["encoder.layer.5.attention.output.LayerNorm.weight", [768]], ["encoder.layer.5.attention.output.LayerNorm.bias", [768]], ["encoder.layer.5.intermediate.dense.weight", [3072, 768]], ["encoder.layer.5.intermediate.dense.bias", [3072]], ["encoder.layer.5.output.dense.weight", [768, 3072]], ["encoder.layer.5.output.dense.bias", [768]], ["encoder.layer.5.output.LayerNorm.weight", [768]], ["encoder.layer.5.output.LayerNorm.bias", [768]], ["encoder.layer.6.attention.self.query.weight", [768, 768]], ["encoder.layer.6.attention.self.query.bias", [768]], ["encoder.layer.6.attention.self.key.weight", [768, 768]], ["encoder.layer.6.attention.self.key.bias", [768]], ["encoder.layer.6.attention.self.value.weight", [768, 768]], ["encoder.layer.6.attention.self.value.bias", [768]], ["encoder.layer.6.attention.output.dense.weight", [768, 768]], ["encoder.layer.6.attention.output.dense.bias", [768]], ["encoder.layer.6.attention.output.LayerNorm.weight", [768]], ["encoder.layer.6.attention.output.LayerNorm.bias", [768]], ["encoder.layer.6.intermediate.dense.weight", [3072, 768]], ["encoder.layer.6.intermediate.dense.bias", [3072]], ["encoder.layer.6.output.dense.weight", [768, 3072]], ["encoder.layer.6.output.dense.bias", [768]], ["encoder.layer.6.output.LayerNorm.weight", [768]], ["encoder.layer.6.output.LayerNorm.bias", [768]], ["encoder.layer.7.attention.self.query.weight", [768, 768]], ["encoder.layer.7.attention.self.query.bias", [768]], ["encoder.layer.7.attention.self.key.weight", [768, 768]], ["encoder.layer.7.attention.self.key.bias", [768]], ["encoder.layer.7.attention.self.value.weight", [768, 768]], ["encoder.layer.7.attention.self.value.bias", [768]], ["encoder.layer.7.attention.output.dense.weight", [768, 768]], ["encoder.layer.7.attention.output.dense.bias", [768]], ["encoder.layer.7.attention.output.LayerNorm.weight", [768]], ["encoder.layer.7.attention.output.LayerNorm.bias", [768]], ["encoder.layer.7.intermediate.dense.weight", [3072, 768]], ["encoder.layer.7.intermediate.dense.bias", [3072]], ["encoder.layer.7.output.dense.weight", [768, 3072]], ["encoder.layer.7.output.dense.bias", [768]], ["encoder.layer.7.output.LayerNorm.weight", [768]], ["encoder.layer.7.output.LayerNorm.bias", [768]], ["encoder.layer.8.attention.self.query.weight", [768, 768]], ["encoder.layer.8.attention.self.query.bias", [768]], ["encoder.layer.8.attention.self.key.weight", [768, 768]], ["encoder.layer.8.attention.self.key.bias", [768]], ["encoder.layer.8.attention.self.value.weight", [768, 768]], ["encoder.layer.8.attention.self.value.bias", [768]], ["encoder.layer.8.attention.output.dense.weight", [768, 768]], ["encoder.layer.8.attention.output.dense.bias", [768]], ["encoder.layer.8.attention.output.LayerNorm.weight", [768]], ["encoder.layer.8.attention.output.LayerNorm.bias", [768]], ["encoder.layer.8.intermediate.dense.weight", [3072, 768]], ["encoder.layer.8.intermediate.dense.bias", [3072]], ["encoder.layer.8.output.dense.weight", [768, 3072]], ["encoder.layer.8.output.dense.bias", [768]], ["encoder.layer.8.output.LayerNorm.weight", [768]], ["encoder.layer.8.output.LayerNorm.bias", [768]], ["encoder.layer.9.attention.self.query.weight", [768, 768]], ["encoder.layer.9.attention.self.query.bias", [768]], ["encoder.layer.9.attention.self.key.weight", [768, 768]], ["encoder.layer.9.attention.self.key.bias", [768]], ["encoder.layer.9.attention.self.value.weight", [768, 768]], ["encoder.layer.9.attention.self.value.bias", [768]], ["encoder.layer.9.attention.output.dense.weight", [768, 768]], ["encoder.layer.9.attention.output.dense.bias", [768]], ["encoder.layer.9.attention.output.LayerNorm.weight", [768]], ["encoder.layer.9.attention.output.LayerNorm.bias", [768]], ["encoder.layer.9.intermediate.dense.weight", [3072, 768]], ["encoder.layer.9.intermediate.dense.bias", [3072]], ["encoder.layer.9.output.dense.weight", [768, 3072]], ["encoder.layer.9.output.dense.bias", [768]], ["encoder.layer.9.output.LayerNorm.weight", [768]], ["encoder.layer.9.output.LayerNorm.bias", [768]], ["encoder.layer.10.attention.self.query.weight", [768, 768]], ["encoder.layer.10.attention.self.query.bias", [768]], ["encoder.layer.10.attention.self.key.weight", [768, 768]], ["encoder.layer.10.attention.self.key.bias", [768]], ["encoder.layer.10.attention.self.value.weight", [768, 768]], ["encoder.layer.10.attention.self.value.bias", [768]], ["encoder.layer.10.attention.output.dense.weight", [768, 768]], ["encoder.layer.10.attention.output.dense.bias", [768]], ["encoder.layer.10.attention.output.LayerNorm.weight", [768]], ["encoder.layer.10.attention.output.LayerNorm.bias", [768]], ["encoder.layer.10.intermediate.dense.weight", [3072, 768]], ["encoder.layer.10.intermediate.dense.bias", [3072]], ["encoder.layer.10.output.dense.weight", [768, 3072]], ["encoder.layer.10.output.dense.bias", [768]], ["encoder.layer.10.output.LayerNorm.weight", [768]], ["encoder.layer.10.output.LayerNorm.bias", [768]], ["encoder.layer.11.attention.self.query.weight", [768, 768]], ["encoder.layer.11.attention.self.query.bias", [768]], ["encoder.layer.11.attention.self.key.weight", [768, 768]], ["encoder.layer.11.attention.self.key.bias", [768]], ["encoder.layer.11.attention.self.value.weight", [768, 768]], ["encoder.layer.11.attention.self.value.bias", [768]], ["encoder.layer.11.attention.output.dense.weight", [768, 768]], ["encoder.layer.11.attention.output.dense.bias", [768]], ["encoder.layer.11.attention.output.LayerNorm.weight", [768]], ["encoder.layer.11.attention.output.LayerNorm.bias", [768]], ["encoder.layer.11.intermediate.dense.weight", [3072, 768]], ["encoder.layer.11.intermediate.dense.bias", [3072]], ["encoder.layer.11.output.dense.weight", [768, 3072]], ["encoder.layer.11.output.dense.bias", [768]], ["encoder.layer.11.output.LayerNorm.weight", [768]], ["encoder.layer.11.output.LayerNorm.bias", [768]], ["pooler.dense.weight", [768, 768]], ["pooler.dense.bias", [768]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], [0], 0, 0, 0, [0], 0, 0, [0], 0, 0, 0, 0]]], "num_parameters": [38603520, 394752, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768]}, {"name": "decoder", "id": 140043324739312, "class_name": "RobertaForCausalLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (crossattention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n  )\n)", "parameters": [["roberta.embeddings.word_embeddings.weight", [50265, 768]], ["roberta.embeddings.position_embeddings.weight", [514, 768]], ["roberta.embeddings.token_type_embeddings.weight", [1, 768]], ["roberta.embeddings.LayerNorm.weight", [768]], ["roberta.embeddings.LayerNorm.bias", [768]], ["roberta.encoder.layer.0.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.0.attention.self.query.bias", [768]], ["roberta.encoder.layer.0.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.0.attention.self.key.bias", [768]], ["roberta.encoder.layer.0.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.0.attention.self.value.bias", [768]], ["roberta.encoder.layer.0.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.0.attention.output.dense.bias", [768]], ["roberta.encoder.layer.0.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.0.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.0.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.0.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.0.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.0.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.0.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.0.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.0.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.0.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.0.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.0.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.0.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.0.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.0.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.0.output.dense.bias", [768]], ["roberta.encoder.layer.0.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.0.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.1.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.1.attention.self.query.bias", [768]], ["roberta.encoder.layer.1.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.1.attention.self.key.bias", [768]], ["roberta.encoder.layer.1.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.1.attention.self.value.bias", [768]], ["roberta.encoder.layer.1.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.1.attention.output.dense.bias", [768]], ["roberta.encoder.layer.1.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.1.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.1.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.1.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.1.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.1.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.1.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.1.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.1.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.1.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.1.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.1.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.1.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.1.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.1.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.1.output.dense.bias", [768]], ["roberta.encoder.layer.1.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.1.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.2.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.2.attention.self.query.bias", [768]], ["roberta.encoder.layer.2.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.2.attention.self.key.bias", [768]], ["roberta.encoder.layer.2.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.2.attention.self.value.bias", [768]], ["roberta.encoder.layer.2.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.2.attention.output.dense.bias", [768]], ["roberta.encoder.layer.2.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.2.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.2.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.2.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.2.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.2.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.2.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.2.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.2.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.2.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.2.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.2.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.2.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.2.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.2.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.2.output.dense.bias", [768]], ["roberta.encoder.layer.2.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.2.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.3.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.3.attention.self.query.bias", [768]], ["roberta.encoder.layer.3.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.3.attention.self.key.bias", [768]], ["roberta.encoder.layer.3.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.3.attention.self.value.bias", [768]], ["roberta.encoder.layer.3.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.3.attention.output.dense.bias", [768]], ["roberta.encoder.layer.3.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.3.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.3.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.3.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.3.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.3.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.3.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.3.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.3.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.3.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.3.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.3.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.3.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.3.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.3.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.3.output.dense.bias", [768]], ["roberta.encoder.layer.3.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.3.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.4.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.4.attention.self.query.bias", [768]], ["roberta.encoder.layer.4.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.4.attention.self.key.bias", [768]], ["roberta.encoder.layer.4.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.4.attention.self.value.bias", [768]], ["roberta.encoder.layer.4.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.4.attention.output.dense.bias", [768]], ["roberta.encoder.layer.4.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.4.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.4.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.4.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.4.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.4.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.4.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.4.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.4.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.4.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.4.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.4.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.4.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.4.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.4.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.4.output.dense.bias", [768]], ["roberta.encoder.layer.4.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.4.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.5.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.5.attention.self.query.bias", [768]], ["roberta.encoder.layer.5.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.5.attention.self.key.bias", [768]], ["roberta.encoder.layer.5.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.5.attention.self.value.bias", [768]], ["roberta.encoder.layer.5.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.5.attention.output.dense.bias", [768]], ["roberta.encoder.layer.5.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.5.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.5.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.5.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.5.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.5.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.5.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.5.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.5.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.5.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.5.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.5.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.5.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.5.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.5.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.5.output.dense.bias", [768]], ["roberta.encoder.layer.5.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.5.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.6.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.6.attention.self.query.bias", [768]], ["roberta.encoder.layer.6.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.6.attention.self.key.bias", [768]], ["roberta.encoder.layer.6.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.6.attention.self.value.bias", [768]], ["roberta.encoder.layer.6.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.6.attention.output.dense.bias", [768]], ["roberta.encoder.layer.6.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.6.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.6.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.6.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.6.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.6.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.6.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.6.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.6.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.6.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.6.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.6.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.6.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.6.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.6.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.6.output.dense.bias", [768]], ["roberta.encoder.layer.6.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.6.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.7.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.7.attention.self.query.bias", [768]], ["roberta.encoder.layer.7.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.7.attention.self.key.bias", [768]], ["roberta.encoder.layer.7.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.7.attention.self.value.bias", [768]], ["roberta.encoder.layer.7.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.7.attention.output.dense.bias", [768]], ["roberta.encoder.layer.7.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.7.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.7.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.7.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.7.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.7.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.7.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.7.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.7.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.7.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.7.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.7.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.7.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.7.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.7.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.7.output.dense.bias", [768]], ["roberta.encoder.layer.7.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.7.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.8.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.8.attention.self.query.bias", [768]], ["roberta.encoder.layer.8.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.8.attention.self.key.bias", [768]], ["roberta.encoder.layer.8.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.8.attention.self.value.bias", [768]], ["roberta.encoder.layer.8.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.8.attention.output.dense.bias", [768]], ["roberta.encoder.layer.8.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.8.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.8.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.8.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.8.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.8.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.8.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.8.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.8.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.8.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.8.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.8.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.8.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.8.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.8.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.8.output.dense.bias", [768]], ["roberta.encoder.layer.8.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.8.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.9.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.9.attention.self.query.bias", [768]], ["roberta.encoder.layer.9.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.9.attention.self.key.bias", [768]], ["roberta.encoder.layer.9.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.9.attention.self.value.bias", [768]], ["roberta.encoder.layer.9.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.9.attention.output.dense.bias", [768]], ["roberta.encoder.layer.9.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.9.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.9.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.9.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.9.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.9.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.9.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.9.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.9.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.9.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.9.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.9.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.9.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.9.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.9.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.9.output.dense.bias", [768]], ["roberta.encoder.layer.9.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.9.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.10.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.10.attention.self.query.bias", [768]], ["roberta.encoder.layer.10.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.10.attention.self.key.bias", [768]], ["roberta.encoder.layer.10.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.10.attention.self.value.bias", [768]], ["roberta.encoder.layer.10.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.10.attention.output.dense.bias", [768]], ["roberta.encoder.layer.10.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.10.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.10.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.10.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.10.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.10.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.10.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.10.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.10.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.10.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.10.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.10.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.10.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.10.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.10.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.10.output.dense.bias", [768]], ["roberta.encoder.layer.10.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.10.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.11.attention.self.query.weight", [768, 768]], ["roberta.encoder.layer.11.attention.self.query.bias", [768]], ["roberta.encoder.layer.11.attention.self.key.weight", [768, 768]], ["roberta.encoder.layer.11.attention.self.key.bias", [768]], ["roberta.encoder.layer.11.attention.self.value.weight", [768, 768]], ["roberta.encoder.layer.11.attention.self.value.bias", [768]], ["roberta.encoder.layer.11.attention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.11.attention.output.dense.bias", [768]], ["roberta.encoder.layer.11.attention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.11.attention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.11.crossattention.self.query.weight", [768, 768]], ["roberta.encoder.layer.11.crossattention.self.query.bias", [768]], ["roberta.encoder.layer.11.crossattention.self.key.weight", [768, 768]], ["roberta.encoder.layer.11.crossattention.self.key.bias", [768]], ["roberta.encoder.layer.11.crossattention.self.value.weight", [768, 768]], ["roberta.encoder.layer.11.crossattention.self.value.bias", [768]], ["roberta.encoder.layer.11.crossattention.output.dense.weight", [768, 768]], ["roberta.encoder.layer.11.crossattention.output.dense.bias", [768]], ["roberta.encoder.layer.11.crossattention.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.11.crossattention.output.LayerNorm.bias", [768]], ["roberta.encoder.layer.11.intermediate.dense.weight", [3072, 768]], ["roberta.encoder.layer.11.intermediate.dense.bias", [3072]], ["roberta.encoder.layer.11.output.dense.weight", [768, 3072]], ["roberta.encoder.layer.11.output.dense.bias", [768]], ["roberta.encoder.layer.11.output.LayerNorm.weight", [768]], ["roberta.encoder.layer.11.output.LayerNorm.bias", [768]], ["lm_head.bias", [50265]], ["lm_head.dense.weight", [768, 768]], ["lm_head.dense.bias", [768]], ["lm_head.layer_norm.weight", [768]], ["lm_head.layer_norm.bias", [768]]], "output_shape": [[[[0], [0], [0], 0], [0, 0, [0], [0], [0], 0]]], "num_parameters": [38603520, 394752, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 50265, 589824, 768, 768, 768]}], "edges": []}