{"format": "torch", "nodes": [{"name": "reformer", "id": 139928573369592, "class_name": "ReformerModel(\n  (embeddings): ReformerEmbeddings(\n    (word_embeddings): Embedding(258, 1024)\n    (position_embeddings): AxialPositionEmbeddings(\n      (weights): ParameterList(\n          (0): Parameter containing: [torch.FloatTensor of size 128x1x256]\n          (1): Parameter containing: [torch.FloatTensor of size 1x512x768]\n      )\n    )\n  )\n  (encoder): ReformerEncoder(\n    (layers): ModuleList(\n      (0): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (1): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (2): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LSHSelfAttention(\n            (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (3): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (4): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (5): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (6): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LSHSelfAttention(\n            (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (7): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (8): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (9): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (10): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LSHSelfAttention(\n            (query_key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n      (11): ReformerLayer(\n        (attention): ReformerAttention(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (self_attention): LocalSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=False)\n            (key): Linear(in_features=1024, out_features=1024, bias=False)\n            (value): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n          (output): ReformerSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n          )\n        )\n        (feed_forward): ChunkReformerFeedForward(\n          (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (dense): ReformerFeedForwardDense(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): ReformerFeedForwardOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          )\n        )\n      )\n    )\n    (layer_norm): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)\n  )\n)", "parameters": [["embeddings.word_embeddings.weight", [258, 1024]], ["embeddings.position_embeddings.weights.0", [128, 1, 256]], ["embeddings.position_embeddings.weights.1", [1, 512, 768]], ["encoder.layers.0.attention.layer_norm.weight", [1024]], ["encoder.layers.0.attention.layer_norm.bias", [1024]], ["encoder.layers.0.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.0.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.0.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.0.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.0.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.0.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.0.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.0.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.0.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.0.feed_forward.output.dense.bias", [1024]], ["encoder.layers.1.attention.layer_norm.weight", [1024]], ["encoder.layers.1.attention.layer_norm.bias", [1024]], ["encoder.layers.1.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.1.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.1.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.1.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.1.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.1.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.1.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.1.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.1.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.1.feed_forward.output.dense.bias", [1024]], ["encoder.layers.2.attention.layer_norm.weight", [1024]], ["encoder.layers.2.attention.layer_norm.bias", [1024]], ["encoder.layers.2.attention.self_attention.query_key.weight", [1024, 1024]], ["encoder.layers.2.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.2.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.2.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.2.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.2.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.2.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.2.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.2.feed_forward.output.dense.bias", [1024]], ["encoder.layers.3.attention.layer_norm.weight", [1024]], ["encoder.layers.3.attention.layer_norm.bias", [1024]], ["encoder.layers.3.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.3.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.3.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.3.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.3.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.3.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.3.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.3.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.3.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.3.feed_forward.output.dense.bias", [1024]], ["encoder.layers.4.attention.layer_norm.weight", [1024]], ["encoder.layers.4.attention.layer_norm.bias", [1024]], ["encoder.layers.4.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.4.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.4.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.4.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.4.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.4.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.4.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.4.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.4.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.4.feed_forward.output.dense.bias", [1024]], ["encoder.layers.5.attention.layer_norm.weight", [1024]], ["encoder.layers.5.attention.layer_norm.bias", [1024]], ["encoder.layers.5.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.5.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.5.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.5.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.5.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.5.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.5.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.5.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.5.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.5.feed_forward.output.dense.bias", [1024]], ["encoder.layers.6.attention.layer_norm.weight", [1024]], ["encoder.layers.6.attention.layer_norm.bias", [1024]], ["encoder.layers.6.attention.self_attention.query_key.weight", [1024, 1024]], ["encoder.layers.6.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.6.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.6.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.6.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.6.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.6.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.6.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.6.feed_forward.output.dense.bias", [1024]], ["encoder.layers.7.attention.layer_norm.weight", [1024]], ["encoder.layers.7.attention.layer_norm.bias", [1024]], ["encoder.layers.7.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.7.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.7.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.7.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.7.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.7.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.7.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.7.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.7.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.7.feed_forward.output.dense.bias", [1024]], ["encoder.layers.8.attention.layer_norm.weight", [1024]], ["encoder.layers.8.attention.layer_norm.bias", [1024]], ["encoder.layers.8.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.8.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.8.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.8.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.8.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.8.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.8.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.8.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.8.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.8.feed_forward.output.dense.bias", [1024]], ["encoder.layers.9.attention.layer_norm.weight", [1024]], ["encoder.layers.9.attention.layer_norm.bias", [1024]], ["encoder.layers.9.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.9.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.9.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.9.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.9.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.9.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.9.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.9.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.9.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.9.feed_forward.output.dense.bias", [1024]], ["encoder.layers.10.attention.layer_norm.weight", [1024]], ["encoder.layers.10.attention.layer_norm.bias", [1024]], ["encoder.layers.10.attention.self_attention.query_key.weight", [1024, 1024]], ["encoder.layers.10.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.10.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.10.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.10.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.10.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.10.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.10.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.10.feed_forward.output.dense.bias", [1024]], ["encoder.layers.11.attention.layer_norm.weight", [1024]], ["encoder.layers.11.attention.layer_norm.bias", [1024]], ["encoder.layers.11.attention.self_attention.query.weight", [1024, 1024]], ["encoder.layers.11.attention.self_attention.key.weight", [1024, 1024]], ["encoder.layers.11.attention.self_attention.value.weight", [1024, 1024]], ["encoder.layers.11.attention.output.dense.weight", [1024, 1024]], ["encoder.layers.11.feed_forward.layer_norm.weight", [1024]], ["encoder.layers.11.feed_forward.layer_norm.bias", [1024]], ["encoder.layers.11.feed_forward.dense.dense.weight", [4096, 1024]], ["encoder.layers.11.feed_forward.dense.dense.bias", [4096]], ["encoder.layers.11.feed_forward.output.dense.weight", [1024, 4096]], ["encoder.layers.11.feed_forward.output.dense.bias", [1024]], ["encoder.layer_norm.weight", [2048]], ["encoder.layer_norm.bias", [2048]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], 0, 0, 0, 0, [0], [0], [0], [0], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], "num_parameters": [264192, 32768, 393216, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1048576, 1048576, 1048576, 1024, 1024, 4194304, 4096, 4194304, 1024, 2048, 2048]}, {"name": "lm_head", "id": 139928559391016, "class_name": "ReformerOnlyLMHead(\n  (decoder): Linear(in_features=2048, out_features=258, bias=True)\n)", "parameters": [["bias", [258]], ["decoder.weight", [258, 2048]]], "output_shape": [[1, 13, 258]], "num_parameters": [258, 528384]}], "edges": []}