{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-sequence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import fastwer\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch.multiprocessing\n",
    "from transformers import RobertaConfig, EncoderDecoderConfig\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.seq2seq import (\n",
    "    Seq2SeqModel,\n",
    "    Seq2SeqArgs,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def count_matches(labels, preds):\n",
    "    return sum(\n",
    "        [\n",
    "            1 if label == pred else 0\n",
    "            for label, pred in zip(labels, preds)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def get_wer(labels, preds):\n",
    "    return np.mean(\n",
    "        [\n",
    "            fastwer.score_sent(pred, label)\n",
    "            for label, pred in zip(labels, preds)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.num_train_epochs = 1\n",
    "# model_args.no_save = True\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = False\n",
    "model_args.tensorboard_dir = \"runs\"\n",
    "model_args.max_length = 50\n",
    "model_args.train_batch_size=20\n",
    "model_args.overwrite_output_dir=True\n",
    "model_args.wandb_project = \"cs224u\"\n",
    "model_args.use_multiprocessed_decoding = True\n",
    "\n",
    "\n",
    "# model = Seq2SeqModel(encoder_decoder_type=\"bart\", encoder_decoder_name=\"./outputs/best_model\", args=model_args, use_cuda=True,)\n",
    "# model = Seq2SeqModel(encoder_type=\"roberta\", encoder_name=\"./outputs/checkpoint-9702-epoch-6/encoder\", decoder_name=\"./outputs/checkpoint-9702-epoch-6/decoder\", args=model_args, config=config, use_cuda=True)\n",
    "\n",
    "model = Seq2SeqModel(\n",
    "    encoder_type=\"bert\",\n",
    "    encoder_name=\"bert-base-uncased\",\n",
    "    decoder_name=\"bert-base-uncased\",\n",
    "    args=model_args,\n",
    "    use_cuda=True,\n",
    ")\n",
    "\n",
    "train_df = pd.read_pickle(\"train.pkl\")\n",
    "train_df = train_df.dropna()\n",
    "dev_df = pd.read_pickle(\"dev.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_df = dev_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-catholic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "wandb.init(project='cs224u', entity='gbanerje')\n",
    "\n",
    "# 2. Save model inputs and hyperparameters\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "\n",
    "# Model training here\n",
    "\n",
    "model.train_model(\n",
    "    train_df, eval_data=dev_df, matches=count_matches, wer=get_wer, show_running_loss=True, args={'fp16': False}\n",
    ")\n",
    "\n",
    "wandb.join()\n",
    "\n",
    "# # Evaluate the model\n",
    "results = model.eval_model(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "        [\n",
    "            \"Hee walks dogks\", \"Hai my precous boi\", \"tteko\", \"e trade often coing sides with other traes\", \"he kepts extensive nodes on a cosing playurs\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "        [\n",
    "            \"Hee woks dogks\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import fastwer\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch.multiprocessing\n",
    "from transformers import RobertaConfig, EncoderDecoderConfig\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import pandas as pd\n",
    "from simpletransformers.seq2seq import (\n",
    "    Seq2SeqModel,\n",
    "    Seq2SeqArgs,\n",
    ")\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.num_train_epochs = 1\n",
    "# model_args.no_save = True\n",
    "model_args.evaluate_generated_text = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = False\n",
    "model_args.tensorboard_dir = \"runs\"\n",
    "model_args.max_length = 50\n",
    "model_args.train_batch_size=10\n",
    "model_args.overwrite_output_dir=True\n",
    "model_args.wandb_project = \"cs224u\"\n",
    "model_args.use_multiprocessed_decoding = True\n",
    "\n",
    "config_encoder = RobertaConfig()\n",
    "# config_decoder = RobertaConfig(is_decoder=True, add_cross_attention=True)\n",
    "config_decoder = RobertaConfig()\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "# model_args = {} #{\"use_multiprocessed_decoding\": True}\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "# encoder_decoder_name = \"roberta\"\n",
    "# model = EncoderDecoderModel(config=config)\n",
    "model = Seq2SeqModel(encoder_type=\"roberta\", encoder_name=\"./outputs/best_model/encoder\", decoder_name=\"./outputs/best_model/decoder\", args=model_args, config=config, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\n",
    "        [\n",
    "            \"the coma sat to te parnting afternoon and the\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-confusion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
