{"format": "torch", "nodes": [{"name": "model", "id": 140685257218368, "class_name": "BartModel(\n  (shared): Embedding(50265, 1024, padding_idx=1)\n  (encoder): BartEncoder(\n    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n    (layers): ModuleList(\n      (0): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): BartEncoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): BartDecoder(\n    (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n    (layers): ModuleList(\n      (0): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (6): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (7): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (8): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (9): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (10): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (11): BartDecoderLayer(\n        (self_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder_attn): BartAttention(\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n        )\n        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n)", "parameters": [["shared.weight", [50265, 1024]], ["encoder.embed_positions.weight", [1026, 1024]], ["encoder.layers.0.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.k_proj.bias", [1024]], ["encoder.layers.0.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.v_proj.bias", [1024]], ["encoder.layers.0.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.q_proj.bias", [1024]], ["encoder.layers.0.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.0.self_attn.out_proj.bias", [1024]], ["encoder.layers.0.self_attn_layer_norm.weight", [1024]], ["encoder.layers.0.self_attn_layer_norm.bias", [1024]], ["encoder.layers.0.fc1.weight", [4096, 1024]], ["encoder.layers.0.fc1.bias", [4096]], ["encoder.layers.0.fc2.weight", [1024, 4096]], ["encoder.layers.0.fc2.bias", [1024]], ["encoder.layers.0.final_layer_norm.weight", [1024]], ["encoder.layers.0.final_layer_norm.bias", [1024]], ["encoder.layers.1.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.k_proj.bias", [1024]], ["encoder.layers.1.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.v_proj.bias", [1024]], ["encoder.layers.1.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.q_proj.bias", [1024]], ["encoder.layers.1.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.1.self_attn.out_proj.bias", [1024]], ["encoder.layers.1.self_attn_layer_norm.weight", [1024]], ["encoder.layers.1.self_attn_layer_norm.bias", [1024]], ["encoder.layers.1.fc1.weight", [4096, 1024]], ["encoder.layers.1.fc1.bias", [4096]], ["encoder.layers.1.fc2.weight", [1024, 4096]], ["encoder.layers.1.fc2.bias", [1024]], ["encoder.layers.1.final_layer_norm.weight", [1024]], ["encoder.layers.1.final_layer_norm.bias", [1024]], ["encoder.layers.2.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.k_proj.bias", [1024]], ["encoder.layers.2.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.v_proj.bias", [1024]], ["encoder.layers.2.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.q_proj.bias", [1024]], ["encoder.layers.2.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.2.self_attn.out_proj.bias", [1024]], ["encoder.layers.2.self_attn_layer_norm.weight", [1024]], ["encoder.layers.2.self_attn_layer_norm.bias", [1024]], ["encoder.layers.2.fc1.weight", [4096, 1024]], ["encoder.layers.2.fc1.bias", [4096]], ["encoder.layers.2.fc2.weight", [1024, 4096]], ["encoder.layers.2.fc2.bias", [1024]], ["encoder.layers.2.final_layer_norm.weight", [1024]], ["encoder.layers.2.final_layer_norm.bias", [1024]], ["encoder.layers.3.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.k_proj.bias", [1024]], ["encoder.layers.3.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.v_proj.bias", [1024]], ["encoder.layers.3.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.q_proj.bias", [1024]], ["encoder.layers.3.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.3.self_attn.out_proj.bias", [1024]], ["encoder.layers.3.self_attn_layer_norm.weight", [1024]], ["encoder.layers.3.self_attn_layer_norm.bias", [1024]], ["encoder.layers.3.fc1.weight", [4096, 1024]], ["encoder.layers.3.fc1.bias", [4096]], ["encoder.layers.3.fc2.weight", [1024, 4096]], ["encoder.layers.3.fc2.bias", [1024]], ["encoder.layers.3.final_layer_norm.weight", [1024]], ["encoder.layers.3.final_layer_norm.bias", [1024]], ["encoder.layers.4.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.k_proj.bias", [1024]], ["encoder.layers.4.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.v_proj.bias", [1024]], ["encoder.layers.4.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.q_proj.bias", [1024]], ["encoder.layers.4.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.4.self_attn.out_proj.bias", [1024]], ["encoder.layers.4.self_attn_layer_norm.weight", [1024]], ["encoder.layers.4.self_attn_layer_norm.bias", [1024]], ["encoder.layers.4.fc1.weight", [4096, 1024]], ["encoder.layers.4.fc1.bias", [4096]], ["encoder.layers.4.fc2.weight", [1024, 4096]], ["encoder.layers.4.fc2.bias", [1024]], ["encoder.layers.4.final_layer_norm.weight", [1024]], ["encoder.layers.4.final_layer_norm.bias", [1024]], ["encoder.layers.5.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.k_proj.bias", [1024]], ["encoder.layers.5.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.v_proj.bias", [1024]], ["encoder.layers.5.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.q_proj.bias", [1024]], ["encoder.layers.5.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.5.self_attn.out_proj.bias", [1024]], ["encoder.layers.5.self_attn_layer_norm.weight", [1024]], ["encoder.layers.5.self_attn_layer_norm.bias", [1024]], ["encoder.layers.5.fc1.weight", [4096, 1024]], ["encoder.layers.5.fc1.bias", [4096]], ["encoder.layers.5.fc2.weight", [1024, 4096]], ["encoder.layers.5.fc2.bias", [1024]], ["encoder.layers.5.final_layer_norm.weight", [1024]], ["encoder.layers.5.final_layer_norm.bias", [1024]], ["encoder.layers.6.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.k_proj.bias", [1024]], ["encoder.layers.6.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.v_proj.bias", [1024]], ["encoder.layers.6.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.q_proj.bias", [1024]], ["encoder.layers.6.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.6.self_attn.out_proj.bias", [1024]], ["encoder.layers.6.self_attn_layer_norm.weight", [1024]], ["encoder.layers.6.self_attn_layer_norm.bias", [1024]], ["encoder.layers.6.fc1.weight", [4096, 1024]], ["encoder.layers.6.fc1.bias", [4096]], ["encoder.layers.6.fc2.weight", [1024, 4096]], ["encoder.layers.6.fc2.bias", [1024]], ["encoder.layers.6.final_layer_norm.weight", [1024]], ["encoder.layers.6.final_layer_norm.bias", [1024]], ["encoder.layers.7.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.k_proj.bias", [1024]], ["encoder.layers.7.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.v_proj.bias", [1024]], ["encoder.layers.7.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.q_proj.bias", [1024]], ["encoder.layers.7.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.7.self_attn.out_proj.bias", [1024]], ["encoder.layers.7.self_attn_layer_norm.weight", [1024]], ["encoder.layers.7.self_attn_layer_norm.bias", [1024]], ["encoder.layers.7.fc1.weight", [4096, 1024]], ["encoder.layers.7.fc1.bias", [4096]], ["encoder.layers.7.fc2.weight", [1024, 4096]], ["encoder.layers.7.fc2.bias", [1024]], ["encoder.layers.7.final_layer_norm.weight", [1024]], ["encoder.layers.7.final_layer_norm.bias", [1024]], ["encoder.layers.8.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.k_proj.bias", [1024]], ["encoder.layers.8.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.v_proj.bias", [1024]], ["encoder.layers.8.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.q_proj.bias", [1024]], ["encoder.layers.8.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.8.self_attn.out_proj.bias", [1024]], ["encoder.layers.8.self_attn_layer_norm.weight", [1024]], ["encoder.layers.8.self_attn_layer_norm.bias", [1024]], ["encoder.layers.8.fc1.weight", [4096, 1024]], ["encoder.layers.8.fc1.bias", [4096]], ["encoder.layers.8.fc2.weight", [1024, 4096]], ["encoder.layers.8.fc2.bias", [1024]], ["encoder.layers.8.final_layer_norm.weight", [1024]], ["encoder.layers.8.final_layer_norm.bias", [1024]], ["encoder.layers.9.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.k_proj.bias", [1024]], ["encoder.layers.9.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.v_proj.bias", [1024]], ["encoder.layers.9.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.q_proj.bias", [1024]], ["encoder.layers.9.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.9.self_attn.out_proj.bias", [1024]], ["encoder.layers.9.self_attn_layer_norm.weight", [1024]], ["encoder.layers.9.self_attn_layer_norm.bias", [1024]], ["encoder.layers.9.fc1.weight", [4096, 1024]], ["encoder.layers.9.fc1.bias", [4096]], ["encoder.layers.9.fc2.weight", [1024, 4096]], ["encoder.layers.9.fc2.bias", [1024]], ["encoder.layers.9.final_layer_norm.weight", [1024]], ["encoder.layers.9.final_layer_norm.bias", [1024]], ["encoder.layers.10.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.k_proj.bias", [1024]], ["encoder.layers.10.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.v_proj.bias", [1024]], ["encoder.layers.10.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.q_proj.bias", [1024]], ["encoder.layers.10.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.10.self_attn.out_proj.bias", [1024]], ["encoder.layers.10.self_attn_layer_norm.weight", [1024]], ["encoder.layers.10.self_attn_layer_norm.bias", [1024]], ["encoder.layers.10.fc1.weight", [4096, 1024]], ["encoder.layers.10.fc1.bias", [4096]], ["encoder.layers.10.fc2.weight", [1024, 4096]], ["encoder.layers.10.fc2.bias", [1024]], ["encoder.layers.10.final_layer_norm.weight", [1024]], ["encoder.layers.10.final_layer_norm.bias", [1024]], ["encoder.layers.11.self_attn.k_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.k_proj.bias", [1024]], ["encoder.layers.11.self_attn.v_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.v_proj.bias", [1024]], ["encoder.layers.11.self_attn.q_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.q_proj.bias", [1024]], ["encoder.layers.11.self_attn.out_proj.weight", [1024, 1024]], ["encoder.layers.11.self_attn.out_proj.bias", [1024]], ["encoder.layers.11.self_attn_layer_norm.weight", [1024]], ["encoder.layers.11.self_attn_layer_norm.bias", [1024]], ["encoder.layers.11.fc1.weight", [4096, 1024]], ["encoder.layers.11.fc1.bias", [4096]], ["encoder.layers.11.fc2.weight", [1024, 4096]], ["encoder.layers.11.fc2.bias", [1024]], ["encoder.layers.11.final_layer_norm.weight", [1024]], ["encoder.layers.11.final_layer_norm.bias", [1024]], ["encoder.layernorm_embedding.weight", [1024]], ["encoder.layernorm_embedding.bias", [1024]], ["decoder.embed_positions.weight", [1026, 1024]], ["decoder.layers.0.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.k_proj.bias", [1024]], ["decoder.layers.0.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.v_proj.bias", [1024]], ["decoder.layers.0.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.q_proj.bias", [1024]], ["decoder.layers.0.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.0.self_attn.out_proj.bias", [1024]], ["decoder.layers.0.self_attn_layer_norm.weight", [1024]], ["decoder.layers.0.self_attn_layer_norm.bias", [1024]], ["decoder.layers.0.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.0.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.0.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.0.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.0.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.0.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.0.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.0.fc1.weight", [4096, 1024]], ["decoder.layers.0.fc1.bias", [4096]], ["decoder.layers.0.fc2.weight", [1024, 4096]], ["decoder.layers.0.fc2.bias", [1024]], ["decoder.layers.0.final_layer_norm.weight", [1024]], ["decoder.layers.0.final_layer_norm.bias", [1024]], ["decoder.layers.1.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.k_proj.bias", [1024]], ["decoder.layers.1.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.v_proj.bias", [1024]], ["decoder.layers.1.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.q_proj.bias", [1024]], ["decoder.layers.1.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.1.self_attn.out_proj.bias", [1024]], ["decoder.layers.1.self_attn_layer_norm.weight", [1024]], ["decoder.layers.1.self_attn_layer_norm.bias", [1024]], ["decoder.layers.1.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.1.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.1.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.1.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.1.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.1.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.1.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.1.fc1.weight", [4096, 1024]], ["decoder.layers.1.fc1.bias", [4096]], ["decoder.layers.1.fc2.weight", [1024, 4096]], ["decoder.layers.1.fc2.bias", [1024]], ["decoder.layers.1.final_layer_norm.weight", [1024]], ["decoder.layers.1.final_layer_norm.bias", [1024]], ["decoder.layers.2.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.k_proj.bias", [1024]], ["decoder.layers.2.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.v_proj.bias", [1024]], ["decoder.layers.2.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.q_proj.bias", [1024]], ["decoder.layers.2.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.2.self_attn.out_proj.bias", [1024]], ["decoder.layers.2.self_attn_layer_norm.weight", [1024]], ["decoder.layers.2.self_attn_layer_norm.bias", [1024]], ["decoder.layers.2.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.2.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.2.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.2.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.2.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.2.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.2.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.2.fc1.weight", [4096, 1024]], ["decoder.layers.2.fc1.bias", [4096]], ["decoder.layers.2.fc2.weight", [1024, 4096]], ["decoder.layers.2.fc2.bias", [1024]], ["decoder.layers.2.final_layer_norm.weight", [1024]], ["decoder.layers.2.final_layer_norm.bias", [1024]], ["decoder.layers.3.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.k_proj.bias", [1024]], ["decoder.layers.3.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.v_proj.bias", [1024]], ["decoder.layers.3.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.q_proj.bias", [1024]], ["decoder.layers.3.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.3.self_attn.out_proj.bias", [1024]], ["decoder.layers.3.self_attn_layer_norm.weight", [1024]], ["decoder.layers.3.self_attn_layer_norm.bias", [1024]], ["decoder.layers.3.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.3.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.3.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.3.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.3.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.3.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.3.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.3.fc1.weight", [4096, 1024]], ["decoder.layers.3.fc1.bias", [4096]], ["decoder.layers.3.fc2.weight", [1024, 4096]], ["decoder.layers.3.fc2.bias", [1024]], ["decoder.layers.3.final_layer_norm.weight", [1024]], ["decoder.layers.3.final_layer_norm.bias", [1024]], ["decoder.layers.4.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.k_proj.bias", [1024]], ["decoder.layers.4.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.v_proj.bias", [1024]], ["decoder.layers.4.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.q_proj.bias", [1024]], ["decoder.layers.4.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.4.self_attn.out_proj.bias", [1024]], ["decoder.layers.4.self_attn_layer_norm.weight", [1024]], ["decoder.layers.4.self_attn_layer_norm.bias", [1024]], ["decoder.layers.4.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.4.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.4.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.4.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.4.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.4.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.4.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.4.fc1.weight", [4096, 1024]], ["decoder.layers.4.fc1.bias", [4096]], ["decoder.layers.4.fc2.weight", [1024, 4096]], ["decoder.layers.4.fc2.bias", [1024]], ["decoder.layers.4.final_layer_norm.weight", [1024]], ["decoder.layers.4.final_layer_norm.bias", [1024]], ["decoder.layers.5.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.k_proj.bias", [1024]], ["decoder.layers.5.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.v_proj.bias", [1024]], ["decoder.layers.5.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.q_proj.bias", [1024]], ["decoder.layers.5.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.5.self_attn.out_proj.bias", [1024]], ["decoder.layers.5.self_attn_layer_norm.weight", [1024]], ["decoder.layers.5.self_attn_layer_norm.bias", [1024]], ["decoder.layers.5.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.5.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.5.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.5.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.5.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.5.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.5.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.5.fc1.weight", [4096, 1024]], ["decoder.layers.5.fc1.bias", [4096]], ["decoder.layers.5.fc2.weight", [1024, 4096]], ["decoder.layers.5.fc2.bias", [1024]], ["decoder.layers.5.final_layer_norm.weight", [1024]], ["decoder.layers.5.final_layer_norm.bias", [1024]], ["decoder.layers.6.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.k_proj.bias", [1024]], ["decoder.layers.6.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.v_proj.bias", [1024]], ["decoder.layers.6.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.q_proj.bias", [1024]], ["decoder.layers.6.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.6.self_attn.out_proj.bias", [1024]], ["decoder.layers.6.self_attn_layer_norm.weight", [1024]], ["decoder.layers.6.self_attn_layer_norm.bias", [1024]], ["decoder.layers.6.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.6.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.6.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.6.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.6.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.6.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.6.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.6.fc1.weight", [4096, 1024]], ["decoder.layers.6.fc1.bias", [4096]], ["decoder.layers.6.fc2.weight", [1024, 4096]], ["decoder.layers.6.fc2.bias", [1024]], ["decoder.layers.6.final_layer_norm.weight", [1024]], ["decoder.layers.6.final_layer_norm.bias", [1024]], ["decoder.layers.7.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.k_proj.bias", [1024]], ["decoder.layers.7.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.v_proj.bias", [1024]], ["decoder.layers.7.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.q_proj.bias", [1024]], ["decoder.layers.7.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.7.self_attn.out_proj.bias", [1024]], ["decoder.layers.7.self_attn_layer_norm.weight", [1024]], ["decoder.layers.7.self_attn_layer_norm.bias", [1024]], ["decoder.layers.7.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.7.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.7.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.7.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.7.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.7.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.7.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.7.fc1.weight", [4096, 1024]], ["decoder.layers.7.fc1.bias", [4096]], ["decoder.layers.7.fc2.weight", [1024, 4096]], ["decoder.layers.7.fc2.bias", [1024]], ["decoder.layers.7.final_layer_norm.weight", [1024]], ["decoder.layers.7.final_layer_norm.bias", [1024]], ["decoder.layers.8.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.k_proj.bias", [1024]], ["decoder.layers.8.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.v_proj.bias", [1024]], ["decoder.layers.8.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.q_proj.bias", [1024]], ["decoder.layers.8.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.8.self_attn.out_proj.bias", [1024]], ["decoder.layers.8.self_attn_layer_norm.weight", [1024]], ["decoder.layers.8.self_attn_layer_norm.bias", [1024]], ["decoder.layers.8.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.8.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.8.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.8.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.8.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.8.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.8.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.8.fc1.weight", [4096, 1024]], ["decoder.layers.8.fc1.bias", [4096]], ["decoder.layers.8.fc2.weight", [1024, 4096]], ["decoder.layers.8.fc2.bias", [1024]], ["decoder.layers.8.final_layer_norm.weight", [1024]], ["decoder.layers.8.final_layer_norm.bias", [1024]], ["decoder.layers.9.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.k_proj.bias", [1024]], ["decoder.layers.9.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.v_proj.bias", [1024]], ["decoder.layers.9.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.q_proj.bias", [1024]], ["decoder.layers.9.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.9.self_attn.out_proj.bias", [1024]], ["decoder.layers.9.self_attn_layer_norm.weight", [1024]], ["decoder.layers.9.self_attn_layer_norm.bias", [1024]], ["decoder.layers.9.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.9.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.9.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.9.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.9.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.9.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.9.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.9.fc1.weight", [4096, 1024]], ["decoder.layers.9.fc1.bias", [4096]], ["decoder.layers.9.fc2.weight", [1024, 4096]], ["decoder.layers.9.fc2.bias", [1024]], ["decoder.layers.9.final_layer_norm.weight", [1024]], ["decoder.layers.9.final_layer_norm.bias", [1024]], ["decoder.layers.10.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.k_proj.bias", [1024]], ["decoder.layers.10.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.v_proj.bias", [1024]], ["decoder.layers.10.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.q_proj.bias", [1024]], ["decoder.layers.10.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.10.self_attn.out_proj.bias", [1024]], ["decoder.layers.10.self_attn_layer_norm.weight", [1024]], ["decoder.layers.10.self_attn_layer_norm.bias", [1024]], ["decoder.layers.10.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.10.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.10.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.10.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.10.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.10.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.10.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.10.fc1.weight", [4096, 1024]], ["decoder.layers.10.fc1.bias", [4096]], ["decoder.layers.10.fc2.weight", [1024, 4096]], ["decoder.layers.10.fc2.bias", [1024]], ["decoder.layers.10.final_layer_norm.weight", [1024]], ["decoder.layers.10.final_layer_norm.bias", [1024]], ["decoder.layers.11.self_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.k_proj.bias", [1024]], ["decoder.layers.11.self_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.v_proj.bias", [1024]], ["decoder.layers.11.self_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.q_proj.bias", [1024]], ["decoder.layers.11.self_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.11.self_attn.out_proj.bias", [1024]], ["decoder.layers.11.self_attn_layer_norm.weight", [1024]], ["decoder.layers.11.self_attn_layer_norm.bias", [1024]], ["decoder.layers.11.encoder_attn.k_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.k_proj.bias", [1024]], ["decoder.layers.11.encoder_attn.v_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.v_proj.bias", [1024]], ["decoder.layers.11.encoder_attn.q_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.q_proj.bias", [1024]], ["decoder.layers.11.encoder_attn.out_proj.weight", [1024, 1024]], ["decoder.layers.11.encoder_attn.out_proj.bias", [1024]], ["decoder.layers.11.encoder_attn_layer_norm.weight", [1024]], ["decoder.layers.11.encoder_attn_layer_norm.bias", [1024]], ["decoder.layers.11.fc1.weight", [4096, 1024]], ["decoder.layers.11.fc1.bias", [4096]], ["decoder.layers.11.fc2.weight", [1024, 4096]], ["decoder.layers.11.fc2.bias", [1024]], ["decoder.layers.11.final_layer_norm.weight", [1024]], ["decoder.layers.11.final_layer_norm.bias", [1024]], ["decoder.layernorm_embedding.weight", [1024]], ["decoder.layernorm_embedding.bias", [1024]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], 0, 0, 0, 0, [0], 0, [0], 0, [0], 0, 0, [0], 0, 0], [0, 0, [0], [0], 0, 0, [0], 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], "num_parameters": [51471360, 1050624, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1024, 1024, 1050624, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1024, 1024]}, {"name": "lm_head", "id": 140685078382800, "class_name": "Linear(in_features=1024, out_features=50265, bias=False)", "parameters": [["weight", [50265, 1024]]], "output_shape": [[10, 127, 50265]], "num_parameters": [51471360]}], "edges": []}